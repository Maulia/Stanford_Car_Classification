# -*- coding: utf-8 -*-
"""car_classify.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ckc4zC1_uZgxLnFBLHptW_Wika9LV_2q

## Maulia Harjono

# DATA



> Download data dari dataset stanford menggunakan perintah wget dan disimpan dalam local storage google colab. Dataset yang di-download adalah cars_train.tgz, cars_test.tgz, dan cars_devkit.tgz. Dataset berupa gambar mobil yang memiliki label sebanyak 196 kelas.
"""

!wget http://imagenet.stanford.edu/internal/car196/cars_train.tgz

!wget http://imagenet.stanford.edu/internal/car196/cars_test.tgz

!wget http://ai.stanford.edu/~jkrause/cars/car_devkit.tgz

!pip3 install console-progressbar

"""# Car Classification



> Klasifikasi dilakukan menggunakan dataset stanford cars. Dataset di-extract dan dipindahkan ke dalam folder data yang terbagi menjadi data train, data test, dan data valid. Ekstraksi, penyimpanan, dan pembacaan dilakukan menggunakan fungsi di bawah.
"""

# This preprocessing portion of the code is provided by foamliu on his github repo
# https://github.com/foamliu/Car-Recognition/blob/master/pre-process.py

import tarfile
import scipy.io
import numpy as np
import os
import cv2 as cv
import shutil
import random
from console_progressbar import ProgressBar


def ensure_folder(folder):
    if not os.path.exists(folder):
        os.makedirs(folder)


def save_train_data(fnames, labels, bboxes):
    src_folder ='//content//cars_train//'
    num_samples = len(fnames)

    train_split = 0.8
    num_train = int(round(num_samples * train_split))
    train_indexes = random.sample(range(num_samples), num_train)

    pb = ProgressBar(total=100, prefix='Save train data', suffix='', decimals=3, length=50, fill='=')

    for i in range(num_samples):
        fname = fnames[i]
        label = labels[i]
        (x1, y1, x2, y2) = bboxes[i]

        src_path = os.path.join(src_folder, fname)
        src_image = cv.imread(src_path)
        height, width = src_image.shape[:2]
        # margins of 16 pixels
        margin = 16
        x1 = max(0, x1 - margin)
        y1 = max(0, y1 - margin)
        x2 = min(x2 + margin, width)
        y2 = min(y2 + margin, height)
        # print("{} -> {}".format(fname, label))
        pb.print_progress_bar((i + 1) * 100 / num_samples)

        if i in train_indexes:
            dst_folder = '//content//data//train//'
        else:
            dst_folder = '//content//data//valid//'

        dst_path = os.path.join(dst_folder, label)
        if not os.path.exists(dst_path):
            os.makedirs(dst_path)
        dst_path = os.path.join(dst_path, fname)

        crop_image = src_image[y1:y2, x1:x2]
        dst_img = cv.resize(src=crop_image, dsize=(img_height, img_width))
        cv.imwrite(dst_path, dst_img)


def save_test_data(fnames, bboxes):
    src_folder = '//content//cars_test//'
    dst_folder = '//content//data//test//'
    num_samples = len(fnames)

    pb = ProgressBar(total=100, prefix='Save test data', suffix='', decimals=3, length=50, fill='=')

    for i in range(num_samples):
        fname = fnames[i]
        (x1, y1, x2, y2) = bboxes[i]
        src_path = os.path.join(src_folder, fname)
        src_image = cv.imread(src_path)
        height, width = src_image.shape[:2]
        # margins of 16 pixels
        margin = 16
        x1 = max(0, x1 - margin)
        y1 = max(0, y1 - margin)
        x2 = min(x2 + margin, width)
        y2 = min(y2 + margin, height)
        # print(fname)
        pb.print_progress_bar((i + 1) * 100 / num_samples)

        dst_path = os.path.join(dst_folder, fname)
        crop_image = src_image[y1:y2, x1:x2]
        dst_img = cv.resize(src=crop_image, dsize=(img_height, img_width))
        cv.imwrite(dst_path, dst_img)


def process_train_data():
    print("Processing train data...")
    cars_annos = scipy.io.loadmat('//content//devkit//cars_train_annos.mat')
    annotations = cars_annos['annotations']
    annotations = np.transpose(annotations)

    fnames = []
    class_ids = []
    bboxes = []
    labels = []

    for annotation in annotations:
        bbox_x1 = annotation[0][0][0][0]
        bbox_y1 = annotation[0][1][0][0]
        bbox_x2 = annotation[0][2][0][0]
        bbox_y2 = annotation[0][3][0][0]
        class_id = annotation[0][4][0][0]
        labels.append('%04d' % (class_id,))
        fname = annotation[0][5][0]
        bboxes.append((bbox_x1, bbox_y1, bbox_x2, bbox_y2))
        class_ids.append(class_id)
        fnames.append(fname)

    labels_count = np.unique(class_ids).shape[0]
    print(np.unique(class_ids))
    print('The number of different cars is %d' % labels_count)

    save_train_data(fnames, labels, bboxes)


def process_test_data():
    print("Processing test data...")
    cars_annos = scipy.io.loadmat('//content//devkit//cars_test_annos.mat')
    annotations = cars_annos['annotations']
    annotations = np.transpose(annotations)

    fnames = []
    bboxes = []

    for annotation in annotations:
        bbox_x1 = annotation[0][0][0][0]
        bbox_y1 = annotation[0][1][0][0]
        bbox_x2 = annotation[0][2][0][0]
        bbox_y2 = annotation[0][3][0][0]
        fname = annotation[0][4][0]
        bboxes.append((bbox_x1, bbox_y1, bbox_x2, bbox_y2))
        fnames.append(fname)

    save_test_data(fnames, bboxes)


if __name__ == '__main__':
    # parameters
    img_width, img_height = 224, 224

    print('Extracting cars_train.tgz...')
    if not os.path.exists('cars_train'):
        with tarfile.open('//content//cars_train.tgz', "r:gz") as tar:
            tar.extractall()
    print('Extracting cars_test.tgz...')
    if not os.path.exists('cars_test'):
        with tarfile.open('//content//cars_test.tgz', "r:gz") as tar:
            tar.extractall()
    print('Extracting car_devkit.tgz...')
    if not os.path.exists('devkit'):
        with tarfile.open('//content//car_devkit.tgz', "r:gz") as tar:
            tar.extractall()

    cars_meta = scipy.io.loadmat('devkit/cars_meta')
    class_names = cars_meta['class_names']  # shape=(1, 196)
    class_names = np.transpose(class_names)
    print('class_names.shape: ' + str(class_names.shape))
    print('Sample class_name: [{}]'.format(class_names[8][0][0]))

    ensure_folder('//content//data/train')
    ensure_folder('//content//data/valid')
    ensure_folder('//content//data/test')

    process_train_data()
    process_test_data()

    # clean up
    shutil.rmtree('cars_train')
    shutil.rmtree('cars_test')
# shutil.rmtree('devkit')

"""# Keras


> Percobaan pertama dilakukan menggunakan CNN yang dibangun dengan Keras. Model CNN yang dibuat terdiri dari 5 layer dengan fungsi aktivasi sigmoid dan relu.



> Data yang digunakan berasal dari data cars_test dengan pembagian 80:20 untuk training dan testing. Masing-masing disimpan dalam file train untuk training dan file valid untuk testing.




> Model di-compile menggunakan optimizer berbeda.
Optimizer pertama adalah Adam dengan loss function = categorical_crossentropy sedangkan optimizer kedua adalah SGD(Stochastic Gradient Descent) dengan loss function = mean_squared_error. Setelah model di-compile, dilakukan training data dengan epoch = 5 dan batch_size = 25. Akurasi yang dihasilkan model dengan Adam optimizer dan SGD optimizer sama, yaitu sebesar 0.92%. Namun loss function yang dihasilkan mean_squared_error lebih kecil daripada categorical_crossentropy. 




> Terakhir dilakukan prediksi dengan menggunakan data tes yang ada di folder cars_test. Setiap gambar yang ada dalam folder tersebut dikategorikan sebagai kelas 100. Hal ini menandakan bahwa model yang dibangun belum mampu memprediksi secara akurat.
"""

import numpy as np
import cv2
import matplotlib.pyplot as plt
import glob
import os
import keras

from keras.models import Sequential
from keras import models, layers
from keras.utils import np_utils
from keras.layers.normalization import BatchNormalization
from keras.layers import Dense, InputLayer, Dropout
from keras.layers import Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.optimizers import SGD, Adam
from sklearn.metrics import confusion_matrix

pics, labels = [], []
i = 0

for dir_path in glob.glob("//content//data//train//*"):
    for pic in glob.glob(os.path.join(dir_path, "*.jpg")):
        temp = cv2.imread(pic)
        temp = cv2.resize(temp, (224, 224))
        pics.append(temp)
        labels.append(i)
    i = i + 1
X_train = np.array(pics)
y = np.array(labels)

y_train = np_utils.to_categorical(y) # mengubah label yang bentuknya teks menjadi angka (0, 1, 2, dst)
print(X_train.shape)
print(y_train.shape)

# menyiapkan validation data untuk training

pics, labels = [], []
i = 0

for dir_path in glob.glob("//content//data//valid//*"):
    for pic in glob.glob(os.path.join(dir_path, "*.jpg")):
        temp = cv2.imread(pic)
        temp = cv2.resize(temp, (224, 224))
        pics.append(temp)
        labels.append(i)
    i = i + 1
X = np.array(pics)
y = np.array(labels)

dummy_y = np_utils.to_categorical(y) # mengubah label yang bentuknya teks menjadi angka (0, 1, 2, dst)
print(X.shape)
print(dummy_y.shape)

X = X/X.max() # centering the data
dummy_y = dummy_y/X.max()

# menyiapkan data testing

pics, labels = [], []
i = 0

for dir_path in glob.glob("//content//data//test//"):
    for pic in glob.glob(os.path.join(dir_path, "*.jpg")):
        temp = cv2.imread(pic)
        temp = cv2.resize(temp, (224, 224))
        pics.append(temp)
        labels.append(i)
    i = i + 1
X_test = np.array(pics)
y_test = np.array(labels)

y_test = np_utils.to_categorical(y_test) # mengubah label yang bentuknya teks menjadi angka (0, 1, 2, dst)
print(X_test.shape)
print(y_test.shape)
# test_img_paths_s

print(X.shape)
print(dummy_y.shape)
print(X_test.shape)
print(y_test.shape)

#Membangun cnn model dengan 5 layer

base_model = Sequential() # modelnya sequential
base_model.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu', input_shape = (224, 224, 3))) # layer convolution
base_model.add(MaxPooling2D(pool_size = (2,2))) # max pooling
base_model.add(Conv2D(64, (3, 3), activation = 'sigmoid')) # layer convolution
base_model.add(MaxPooling2D(pool_size = (2,2))) # max pooling
base_model.add(Conv2D(64, (3, 3), activation = 'relu')) # layer convolution
base_model.add(MaxPooling2D(pool_size = (2,2))) # max pooling
base_model.add(Conv2D(32, (3, 3), activation = 'sigmoid')) # layer convolution
base_model.add(MaxPooling2D(pool_size = (2,2))) # max pooling
base_model.add(Conv2D(32, (3, 3), activation = 'relu')) # layer convolution
base_model.add(MaxPooling2D(pool_size = (2,2))) # max pooling
base_model.add(Dropout(0.3)) # mengurangi overfitting
base_model.add(Flatten())
base_model.add(Dense(512, activation = 'relu')) # masuk ke neural networknya
base_model.add(Dense(256, activation = 'softmax')) # masuk ke neural networknya
base_model.add(Dropout(0.5)) # biasanya ini udah max
base_model.add(Dense(196, activation = 'softmax')) # outputnya 196 sesuai dengan jumlah labelnya dan biasanya klasifikasi terakhir pake softmax activation function

base_model.summary()

# Compiling the model
base_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

# Training the model
base_model.fit(X_train, y_train, epochs = 5, verbose = 1, validation_data = (X, dummy_y), batch_size = 25) 
# batch size itu di sini berarti tiap 25 data, weightnya di update sampe mencapai 1 epoch.

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

scores = base_model.evaluate(X, dummy_y, verbose = 0)
print("Accuracy: %.2f%%" % (scores[1] * 100))

# Compiling the model
base_model.compile(loss = 'mean_squared_error', optimizer = 'sgd', metrics = ['accuracy'])

# Training the model
base_model.fit(X_train, y_train, epochs = 5, verbose = 1, validation_data = (X, dummy_y), batch_size = 25) 
# batch size itu di sini berarti tiap 25 data, weightnya di update sampe mencapai 1 epoch.

scores = base_model.evaluate(X, dummy_y, verbose = 1)
print("Accuracy: %.2f%%" % (scores[1] * 100))

#Prediksi menggunakan data test

prediction = base_model.predict_classes(X)
prediction

"""# FASTAI



> Percobaan kedua dilakukan menggunakan fastai yang menerapkan resnet50 sebagai CNN feature extraction. Data yang digunakan sama dengan yang diterapkan pada Kera, yaitu berasal dari data cars_test dengan pembagian 80:20 untuk training dan testing. Masing-masing disimpan dalam file train untuk training dan file valid untuk testing.
"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline
from fastai.vision import *
from fastai.metrics import error_rate
from fastai import *
import cv2 as cv
import numpy as np
import pandas as pd
import scipy.io as sio

#Mengambil data train dan menyimpannya dalam image data bunch.
data = ImageDataBunch.from_folder('//content//data//','train','valid',ds_tfms=get_transforms(do_flip=False, flip_vert=True, max_rotate=5.0, max_zoom=1.1, max_lighting=0.2, max_warp=0.2, p_affine=0.75, p_lighting=0.75),size=224,bs=32).normalize(imagenet_stats)

data.show_batch()

# nama kelas dan jumlah kelas
print(data.classes)
len(data.classes),data.c

#Training menggunakan resnet50
learn = cnn_learner(data, models.resnet50, metrics=accuracy)

#Training sebanyak 5 epoch
learn.fit_one_cycle(5)

preds,y, loss = learn.get_preds(with_loss=True)
acc = accuracy(preds, y)
print('The accuracy is {0} %.'.format(acc))

"""> Akurasi yang didapatkan menggunakan resnet50 dengan epoch = 5 dan batch size = 32 adalah 90%"""

learn.recorder.plot_losses()

#simpan model
learn.save('stanford-cars-1')

#Lakukan interpretasi untuk memperbaiki prediksi
interp = ClassificationInterpretation.from_learner(learn)

losses,idxs = interp.top_losses()

len(data.valid_ds)==len(losses)==len(idxs)

#Berikut adalah 9 gambar dengan kesalahan prediksi
interp.plot_top_losses(9, figsize=(15,11))

learn.lr_find()

learn.recorder.plot()

#unfreeze model dan lakukan training lagi dengan 20 epoch
learn.unfreeze()
learn.fit_one_cycle(20, max_lr=slice(1e-5,1e-4))

learn.recorder.plot_losses()

"""> Pada gambar di atas terlihat bahwa terjadi overfitting pada model"""

preds,y, loss = learn.get_preds(with_loss=True)
acc = accuracy(preds, y)
print('The accuracy is {0} %.'.format(acc))

"""> Akurasi yang didapatkan setelah menerapkan classification interpretation dengan epoch = 20 dan batch size = 32 adalah 98%

# Kesimpulan



> CNN yang dibangun dengan Keras baik menggunakan Adam optimizer atau SGD optimizer hanya berhasil menghasilkan akurasi sebesar 0.92%. Sedangkan CNN yang dibangun dengan Fastai menggunakan resnet50 berhasil menghasilkan akurasi sebesar 90% dan setelah dilakukan Classification Interpretation, akurasi meningkat hingga mencapai 98%.
"""